{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the neccesary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For purpose of this problem, we load the data given as spambase.data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=pd.read_csv(\"spambase.data\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just In case we need the above file for future computations, we create a copy and actually perform every computation on this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=file.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us check our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000  ...   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413  ...   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000  ...   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000  ...   \n",
       "\n",
       "                48           49           50           51           52  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.038575     0.139030     0.016976     0.269071     0.075811   \n",
       "std       0.243471     0.270355     0.109394     0.815672     0.245882   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.065000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.188000     0.000000     0.315000     0.052000   \n",
       "max       4.385000     9.752000     4.081000    32.478000     6.003000   \n",
       "\n",
       "                53           54           55            56           57  \n",
       "count  4601.000000  4601.000000  4601.000000   4601.000000  4601.000000  \n",
       "mean      0.044238     5.191515    52.172789    283.289285     0.394045  \n",
       "std       0.429342    31.729449   194.891310    606.347851     0.488698  \n",
       "min       0.000000     1.000000     1.000000      1.000000     0.000000  \n",
       "25%       0.000000     1.588000     6.000000     35.000000     0.000000  \n",
       "50%       0.000000     2.276000    15.000000     95.000000     0.000000  \n",
       "75%       0.000000     3.706000    43.000000    266.000000     1.000000  \n",
       "max      19.829000  1102.500000  9989.000000  15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know that our dataset has total 4601 entries for emails and 57 features, and our label is 0/1(Not Spam/Spam)\n",
    "From the above we can see that we have no missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive deeper to verify for each column if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total_Missing_value</th>\n",
       "      <th>Missing_value_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Total_Missing_value  Missing_value_%\n",
       "57                    0              0.0\n",
       "14                    0              0.0\n",
       "26                    0              0.0\n",
       "25                    0              0.0\n",
       "24                    0              0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = df.isnull().sum()/df.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total_Missing_value', 'Missing_value_%'])\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the data doesn't have any missing values and it is already pre-processed(No categorical/nominal to numeric conversion required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us check if we can select features for our model and reduce amount of features while not compromisng accuracy by more than 5%\n",
    "##### For this we use the chi-square test(Generally Used for Categorical Attributes but turns numeric attributes into discrete automatically and peforms feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is IMPORTANT for Prediction\n",
      "1 is IMPORTANT for Prediction\n",
      "2 is IMPORTANT for Prediction\n",
      "3 is IMPORTANT for Prediction\n",
      "4 is IMPORTANT for Prediction\n",
      "5 is IMPORTANT for Prediction\n",
      "6 is IMPORTANT for Prediction\n",
      "7 is IMPORTANT for Prediction\n",
      "8 is IMPORTANT for Prediction\n",
      "9 is IMPORTANT for Prediction\n",
      "10 is IMPORTANT for Prediction\n",
      "11 is IMPORTANT for Prediction\n",
      "12 is IMPORTANT for Prediction\n",
      "13 is IMPORTANT for Prediction\n",
      "14 is IMPORTANT for Prediction\n",
      "15 is IMPORTANT for Prediction\n",
      "16 is IMPORTANT for Prediction\n",
      "17 is IMPORTANT for Prediction\n",
      "18 is IMPORTANT for Prediction\n",
      "19 is IMPORTANT for Prediction\n",
      "20 is IMPORTANT for Prediction\n",
      "21 is IMPORTANT for Prediction\n",
      "22 is IMPORTANT for Prediction\n",
      "23 is IMPORTANT for Prediction\n",
      "24 is IMPORTANT for Prediction\n",
      "25 is IMPORTANT for Prediction\n",
      "26 is IMPORTANT for Prediction\n",
      "27 is IMPORTANT for Prediction\n",
      "28 is IMPORTANT for Prediction\n",
      "29 is IMPORTANT for Prediction\n",
      "30 is IMPORTANT for Prediction\n",
      "31 is IMPORTANT for Prediction\n",
      "32 is NOT an important predictor. (Discard 32 from model)\n",
      "33 is IMPORTANT for Prediction\n",
      "34 is IMPORTANT for Prediction\n",
      "35 is IMPORTANT for Prediction\n",
      "36 is IMPORTANT for Prediction\n",
      "37 is NOT an important predictor. (Discard 37 from model)\n",
      "38 is IMPORTANT for Prediction\n",
      "39 is IMPORTANT for Prediction\n",
      "40 is NOT an important predictor. (Discard 40 from model)\n",
      "41 is NOT an important predictor. (Discard 41 from model)\n",
      "42 is IMPORTANT for Prediction\n",
      "43 is NOT an important predictor. (Discard 43 from model)\n",
      "44 is IMPORTANT for Prediction\n",
      "45 is IMPORTANT for Prediction\n",
      "46 is NOT an important predictor. (Discard 46 from model)\n",
      "47 is NOT an important predictor. (Discard 47 from model)\n",
      "48 is IMPORTANT for Prediction\n",
      "49 is IMPORTANT for Prediction\n",
      "50 is NOT an important predictor. (Discard 50 from model)\n",
      "51 is IMPORTANT for Prediction\n",
      "52 is IMPORTANT for Prediction\n",
      "53 is IMPORTANT for Prediction\n",
      "54 is IMPORTANT for Prediction\n",
      "55 is IMPORTANT for Prediction\n",
      "56 is IMPORTANT for Prediction\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "class ChiSquare:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.p = None #P-Value\n",
    "        self.chi2 = None #Chi Test Statistic\n",
    "        self.dof = None\n",
    "        \n",
    "        self.dfObserved = None\n",
    "        self.dfExpected = None\n",
    "        \n",
    "    def _print_chisquare_result(self, colX, alpha):\n",
    "        result = \"\"\n",
    "        if self.p<alpha:\n",
    "            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n",
    "        else:\n",
    "            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n",
    "\n",
    "        print(result)\n",
    "        \n",
    "    def TestIndependence(self,colX,colY, alpha=0.05):\n",
    "        X = self.df[colX].astype(str)\n",
    "        Y = self.df[colY].astype(str)\n",
    "        \n",
    "        self.dfObserved = pd.crosstab(Y,X) \n",
    "        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n",
    "        self.p = p\n",
    "        self.chi2 = chi2\n",
    "        self.dof = dof \n",
    "        \n",
    "        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n",
    "        \n",
    "        self._print_chisquare_result(colX,alpha)\n",
    "\n",
    "\n",
    "#Initialize ChiSquare Class\n",
    "cT = ChiSquare(df)\n",
    "# train_df.columns\n",
    "# Feature Selection\n",
    "col=df.columns[:-1]\n",
    "testColumns = col\n",
    "for var in testColumns:\n",
    "    cT.TestIndependence(colX=var,colY=57 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are 8 features which can be successfully dropped (as they won't be affecting the accuracy of our model by significant amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df.drop(32,axis=1)\n",
    "train_df=train_df.drop(37,axis=1)\n",
    "train_df=train_df.drop(40,axis=1)\n",
    "train_df=train_df.drop(41,axis=1)\n",
    "train_df=train_df.drop(43,axis=1)\n",
    "train_df=train_df.drop(47,axis=1)\n",
    "train_df=train_df.drop(50,axis=1)\n",
    "train_df=train_df.drop(46,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34,\n",
       "            35, 36, 38, 39, 42, 44, 45, 48, 49, 51, 52, 53, 54, 55, 56, 57],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now I am checking whether there is correlation between any of the remaining features present, if there is we can safely drop this correlated feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlated_features = set()  \n",
    "correlation_matrix = train_df.corr()  \n",
    "for i in range(len(correlation_matrix .columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.85:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "(correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.drop(33,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the name of the columns for better inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.rename(columns=lambda x: \"col\"+str(x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "\n",
    "df_train = train_df.drop(\"col57\",axis=1)\n",
    "\n",
    "y =  train_df[\"col57\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data into training and testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(df_train,y,test_size=0.25,random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have pre-processed our data and splitted it into training and testing sets\n",
    "\n",
    "### Building Clasifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier : 1-Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  F-pos   F-neg   Error rate\n",
      "0.8308   0.1362   0.2198   0.1692\n",
      "0.8395   0.1183   0.2253   0.1605\n",
      "0.8113   0.1398   0.2637   0.1887\n",
      "0.8196   0.1470   0.2320   0.1804\n",
      "0.8065   0.1792   0.2155   0.1935\n",
      "0.8348   0.1111   0.2486   0.1652\n",
      "0.8087   0.1613   0.2376   0.1913\n",
      "0.8370   0.1326   0.2099   0.1630\n",
      "0.8627   0.1115   0.1768   0.1373\n",
      "0.8344   0.1619   0.1713   0.1656\n",
      "Overall Error rate : 0.1715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0) \n",
    "\n",
    "print(\"Accuracy  F-pos   F-neg   Error rate\")\n",
    "errors = []\n",
    "\n",
    "for train,test in kfold.split(df_train,y):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "    nfit = neigh.fit(X_train, Y_train)\n",
    "    result1 = nfit.predict(X_test)\n",
    "    accuracy_score(Y_test, result1)\n",
    "    neigh = neigh.fit(df_train.iloc[train],y.iloc[train])\n",
    "    y_pred=neigh.predict(df_train.iloc[test])\n",
    "    score = accuracy_score(y.iloc[test],y_pred)\n",
    "    cm = confusion_matrix(y.iloc[test], y_pred)\n",
    "    fraction_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    errors.append(1-score)\n",
    "    print(\"{0:.4f}   {1:.4f}   {2:.4f}   {3:.4f}\".format(score, fraction_cm[0][1], fraction_cm[1][0], (1-score)))\n",
    "\n",
    "errors = np.array(errors)\n",
    "print(\"Overall Error rate : {0:.4f}\".format(errors.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classifier : 10-Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  F-pos   F-neg   Error rate\n",
      "0.7831   0.1219   0.3626   0.2169\n",
      "0.7983   0.1075   0.3462   0.2017\n",
      "0.7939   0.1362   0.3132   0.2061\n",
      "0.7848   0.1254   0.3536   0.2152\n",
      "0.7826   0.1470   0.3260   0.2174\n",
      "0.7761   0.1183   0.3867   0.2239\n",
      "0.7696   0.1434   0.3646   0.2304\n",
      "0.7739   0.1290   0.3757   0.2261\n",
      "0.7996   0.1223   0.3204   0.2004\n",
      "0.8235   0.1151   0.2707   0.1765\n",
      "Overall Error rate : 0.2115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0) \n",
    "\n",
    "print(\"Accuracy  F-pos   F-neg   Error rate\")\n",
    "errors = []\n",
    "\n",
    "for train,test in kfold.split(df_train,y):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "    nfit = neigh.fit(X_train, Y_train)\n",
    "    result1 = nfit.predict(X_test)\n",
    "    accuracy_score(Y_test, result1)\n",
    "    neigh = neigh.fit(df_train.iloc[train],y.iloc[train])\n",
    "    y_pred=neigh.predict(df_train.iloc[test])\n",
    "    score = accuracy_score(y.iloc[test],y_pred)\n",
    "    cm = confusion_matrix(y.iloc[test], y_pred)\n",
    "    fraction_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    errors.append(1-score)\n",
    "    print(\"{0:.4f}   {1:.4f}   {2:.4f}   {3:.4f}\".format(score, fraction_cm[0][1], fraction_cm[1][0], (1-score)))\n",
    "\n",
    "errors = np.array(errors)\n",
    "print(\"Overall Error rate : {0:.4f}\".format(errors.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 100, test_size = .2, train_size = .6, random_state = 0)\n",
    "dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "base_results = model_selection.cross_validate(dtree, X_train, Y_train, cv  = 10)\n",
    "dtree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (False -pos, False -neg, Error Rate) for all Folds : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  F-pos   F-neg   Error rate\n",
      "0.9197   0.0609   0.1099   0.0803\n",
      "0.9219   0.0789   0.0769   0.0781\n",
      "0.9046   0.0753   0.1264   0.0954\n",
      "0.9283   0.0645   0.0829   0.0717\n",
      "0.9065   0.0753   0.1215   0.0935\n",
      "0.9109   0.0717   0.1160   0.0891\n",
      "0.9152   0.0466   0.1436   0.0848\n",
      "0.9152   0.0753   0.0994   0.0848\n",
      "0.9325   0.0540   0.0884   0.0675\n",
      "0.8998   0.1079   0.0884   0.1002\n",
      "Overall Error rate : 0.0845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0) #Kfold gives overall error of 0.0724\n",
    "\n",
    "print(\"Accuracy  F-pos   F-neg   Error rate\")\n",
    "errors = []\n",
    "\n",
    "for train,test in kfold.split(df_train,y):\n",
    "    dtree = tree.DecisionTreeClassifier(random_state = 0)\n",
    "    dtree = dtree.fit(df_train.iloc[train],y.iloc[train])\n",
    "    y_pred=dtree.predict(df_train.iloc[test])\n",
    "    score = accuracy_score(y.iloc[test],y_pred)\n",
    "    cm = confusion_matrix(y.iloc[test], y_pred)\n",
    "    fraction_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    errors.append(1-score)\n",
    "    print(\"{0:.4f}   {1:.4f}   {2:.4f}   {3:.4f}\".format(score, fraction_cm[0][1], fraction_cm[1][0], (1-score)))\n",
    "\n",
    "errors = np.array(errors)\n",
    "print(\"Overall Error rate : {0:.4f}\".format(errors.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we got 92% accuracy with the decision tress classifier and now let us check crossvalidated scores for each fold to see if we are over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossvalidated scores: [0.8699422  0.88695652 0.91884058 0.91304348 0.92463768 0.89855072\n",
      " 0.92463768 0.93623188 0.91014493 0.88953488]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(dtree, X_train, Y_train, cv=10)\n",
    "print (\"Crossvalidated scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.9322328410078193 \n",
      "\n",
      "\n",
      "Accuracy score :  0.9322328410078193 \n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       697\n",
      "           1       0.92      0.91      0.91       454\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1151\n",
      "   macro avg       0.93      0.93      0.93      1151\n",
      "weighted avg       0.93      0.93      0.93      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Score :',clf.score(X_test,Y_test),'\\n\\n')\n",
    "print('Accuracy score : ', accuracy_score(Y_test,y_pred),'\\n\\n')\n",
    "print('Classification report:\\n',classification_report(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I got an accuracy of 93% for my logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression has better accuracy than above models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (False -pos, False -neg, Error Rate) for all Folds : Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  F-pos   F-neg   Error rate\n",
      "0.9197   0.0394   0.1429   0.0803\n",
      "0.9523   0.0287   0.0769   0.0477\n",
      "0.9046   0.0609   0.1484   0.0954\n",
      "0.9065   0.0824   0.1105   0.0935\n",
      "0.9217   0.0502   0.1215   0.0783\n",
      "0.9174   0.0645   0.1105   0.0826\n",
      "0.9196   0.0287   0.1602   0.0804\n",
      "0.9239   0.0394   0.1326   0.0761\n",
      "0.9281   0.0432   0.1160   0.0719\n",
      "0.9237   0.0647   0.0939   0.0763\n",
      "Overall Error rate : 0.0782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0) #Kfold gives overall error of 0.0724\n",
    "\n",
    "print(\"Accuracy  F-pos   F-neg   Error rate\")\n",
    "errors = []\n",
    "\n",
    "for train,test in kfold.split(df_train,y):\n",
    "    clf = LogisticRegression()\n",
    "    clf = clf.fit(df_train.iloc[train],y.iloc[train])\n",
    "    y_pred_log=clf.predict(df_train.iloc[test])\n",
    "    score = accuracy_score(y.iloc[test],y_pred_log)\n",
    "    cm = confusion_matrix(y.iloc[test], y_pred_log)\n",
    "    fraction_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    errors.append(1-score)\n",
    "    print(\"{0:.4f}   {1:.4f}   {2:.4f}   {3:.4f}\".format(score, fraction_cm[0][1], fraction_cm[1][0], (1-score)))\n",
    "\n",
    "errors = np.array(errors)\n",
    "print(\"Overall Error rate : {0:.4f}\".format(errors.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9331016507384883"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bdt_real = AdaBoostClassifier(DecisionTreeClassifier(random_state = 0),n_estimators=100,learning_rate=0.5)\n",
    "bdt_real.fit(X_train, Y_train)\n",
    "real_test_predict = bdt_real.predict(X_test)\n",
    "accuracy_score(real_test_predict, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossvalidated scores: for ADABOOST [0.86416185 0.95072464 0.95942029 0.93333333 0.96231884 0.91884058\n",
      " 0.95072464 0.93913043 0.90434783 0.89825581]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "scores = cross_val_score(bdt_real, X_train, Y_train, cv=10)\n",
    "print (\"Crossvalidated scores: for ADABOOST\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (False -pos, False -neg, Error Rate) for all Folds : AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  F-pos   F-neg   Error rate\n",
      "0.9436   0.0287   0.0989   0.0564\n",
      "0.9479   0.0538   0.0495   0.0521\n",
      "0.9176   0.0466   0.1374   0.0824\n",
      "0.9283   0.0609   0.0884   0.0717\n",
      "0.9348   0.0430   0.0994   0.0652\n",
      "0.9435   0.0394   0.0829   0.0565\n",
      "0.9478   0.0143   0.1105   0.0522\n",
      "0.9457   0.0323   0.0884   0.0543\n",
      "0.9477   0.0288   0.0884   0.0523\n",
      "0.9216   0.0971   0.0497   0.0784\n",
      "Overall Error rate : 0.0622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0) #Kfold gives overall error of 0.0724\n",
    "\n",
    "print(\"Accuracy  F-pos   F-neg   Error rate\")\n",
    "errors = []\n",
    "\n",
    "\n",
    "for train,test in kfold.split(df_train,y):\n",
    "    bdt_real = AdaBoostClassifier(DecisionTreeClassifier(random_state = 0),n_estimators=100,learning_rate=0.5)\n",
    "    bdt_real = bdt_real.fit(df_train.iloc[train],y.iloc[train])\n",
    "    y_pred=bdt_real.predict(df_train.iloc[test])\n",
    "    score = accuracy_score(y.iloc[test],y_pred)\n",
    "    cm = confusion_matrix(y.iloc[test], y_pred)\n",
    "    fraction_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    errors.append(1-score)\n",
    "    print(\"{0:.4f}   {1:.4f}   {2:.4f}   {3:.4f}\".format(score, fraction_cm[0][1], fraction_cm[1][0], (1-score)))\n",
    "\n",
    "errors = np.array(errors)\n",
    "print(\"Overall Error rate : {0:.4f}\".format(errors.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### AdaBoost gave 94% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9374456993918332"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train)  \n",
    "St_train = scaler.transform(X_train)  \n",
    "# apply same transformation to test data\n",
    "St_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpnn = MLPClassifier(solver='lbfgs', alpha=1e-5,random_state=1)\n",
    "mlpnn.fit(St_train, Y_train)\n",
    "mlp_pred = mlpnn.predict(St_test)\n",
    "accuracy_score(mlp_pred, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (False -pos, False -neg, Error Rate) for all Folds : MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  F-pos   F-neg   Error rate\n",
      "0.8633   0.0968   0.1978   0.1367\n",
      "0.8937   0.0860   0.1374   0.1063\n",
      "0.8286   0.2186   0.0989   0.1714\n",
      "0.8283   0.1864   0.1492   0.1717\n",
      "0.6913   0.2115   0.4586   0.3087\n",
      "0.8587   0.1254   0.1657   0.1413\n",
      "0.7152   0.0932   0.5801   0.2848\n",
      "0.8478   0.1183   0.2044   0.1522\n",
      "0.8540   0.1367   0.1602   0.1460\n",
      "0.8780   0.1403   0.0939   0.1220\n",
      "Overall Error rate : 0.1741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0) #Kfold gives overall error of 0.0724\n",
    "\n",
    "print(\"Accuracy  F-pos   F-neg   Error rate\")\n",
    "errors = []\n",
    "\n",
    "\n",
    "for train,test in kfold.split(df_train,y):\n",
    "    mlpnn = MLPClassifier(solver='lbfgs', alpha=1e-5,random_state=1)\n",
    "    mlpnn = mlpnn.fit(df_train.iloc[train],y.iloc[train])\n",
    "    y_pred=mlpnn.predict(df_train.iloc[test])\n",
    "    score = accuracy_score(y.iloc[test],y_pred)\n",
    "    cm = confusion_matrix(y.iloc[test], y_pred)\n",
    "    fraction_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    errors.append(1-score)\n",
    "    print(\"{0:.4f}   {1:.4f}   {2:.4f}   {3:.4f}\".format(score, fraction_cm[0][1], fraction_cm[1][0], (1-score)))\n",
    "\n",
    "errors = np.array(errors)\n",
    "print(\"Overall Error rate : {0:.4f}\".format(errors.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hence, from above observation we can see that Adaboost, Random Forest and Logistic give us decent accuracies per fold and error rates, while KNN doesn't perform well and MLP Classifier has bad cross-validation scores. I will choose Adaboost as my classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
